# -*- coding: utf-8 -*-
"""TFM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z0Ulc6LH2jFWQnFNAxvbANJnJB9aIJk7

# WEB SCRAPING

Importamos paquetes necesarios
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json

"""Obtenemos las URL de todas las páginas con noticias de la VIU"""

def obtener_enlaces(num_pag):
  j=0
  enlaces_noticias=[]
  enlaces_paginas=[]
  for j in range(0,num_pag):
    if j == 0:
      URL = "https://www.universidadviu.com/es/actualidad/noticias?buscar=&fecha=&field_shared_category_target_id=All&field_shared_tag_target_id=All"
    else:
      URL = "https://www.universidadviu.com/es/actualidad/noticias?buscar=&fecha=&field_shared_category_target_id=All&field_shared_tag_target_id=All&page="+str(j)
    enlaces_paginas.append(URL)
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    enlace_pagina=soup.find_all('div', class_="views-row")
    for i in range(len(enlace_pagina)):
      enlaces_noticias.append("https://www.universidadviu.com"+str(enlace_pagina[i].find("a").attrs["href"]))
  return enlaces_paginas, enlaces_noticias

enlaces_paginas, enlaces_noticias=obtener_enlaces(49)

"""Nos aseguramos de tener todas las páginas detectadas (actualmente a fecha 01/09/2023 la VIU presenta 49 paginas)."""

len(enlaces_paginas)

"""Del mismo modo, nos aseguramos de tener todas las noticias de la VIU detectadas (vemos que OK, pues hay 49 páginas con 16 noticias cada una,por lo que 49x16=784 OK)"""

len(enlaces_noticias)

"""\\Vamos extrayendo el contenido de las diferentes noticias, se añade un contador que se muestra por pantalla, para ir viendo el número de extracciones ya realizadas."""

def obtener_contenido(enlaces_noticias):
  contenidos=[]
  contador=0
  for i in enlaces_noticias:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, "html.parser")
    if soup.find("script", type="application/ld+json")!=None:
      contenidos.append([json.loads(soup.find("script", type="application/ld+json").text)["@graph"][0]["articleBody"]])
      contador+=1
    print(contador)
  return contenidos

contenidos=obtener_contenido(enlaces_noticias)

"""Cada contenido es una lista dentro de la lista "contenidos", de esta forma podemos acceder fácil al contenido y podemos mapearlo con la noticia en cuestión si fuera necesario."""

len(contenidos)

"""Compruebo un contenido al azar"""

contenidos[701]

"""Como hemos dicho el contenido de cada noticias es una lista dentro de "contenidos", en este caso es una lista de longitud 1, luego para poder trabajar sobre el texto propiamente dicho del contenido, debemos poner [0]."""

contenidos[701][0]

"""Veamos el número de caracteres de una noticia al azar"""

len(contenidos[701][0])

"""# PROCESAMIENTO"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import ToktokTokenizer
from gensim.corpora import Dictionary

def preprocesamiento(texto):
    """
    Función para realizar la limpieza de un texto dado.
    """
    # Eliminamos los caracteres especiales
    texto = re.sub(r'\W', ' ', str(texto))
    # Eliminamos las palabras que tengo un solo caracter
    texto = re.sub(r'\s+[a-zA-Z]\s+', ' ', texto)
    # Sustituir los espacios en blanco en uno solo
    texto = re.sub(r'\s+', ' ', texto, flags=re.I)
    #Eliminamos otros espacios en blanco forzados en lenguaje HTML
    texto = re.sub(r'nbsp', ' ', texto, flags=re.I)
    # Convertimos textos a minusculas
    texto = texto.lower()
    return texto

for i in range(len(contenidos)):
  contenidos[i][0]=preprocesamiento(contenidos[i][0])

"""Guardamos los contenidos sin procesar paras los modelos que tienen procesamientos propios."""

contenidosPLSA=contenidos
contenidosLSTM=contenidos

"""Veamos su resultado, sobre el contenido que mostramos con anterioridad (contenido noticia 0)"""

contenidos[701][0]

len(contenidos[701][0])

"""Los espacios ocasionados por el preprocesamiento no son importantes, pues el tokenizar en el paso a continuación desaparecerán."""

tokenizer = ToktokTokenizer()
for i in range(len(contenidos)):
  contenidos[i][0]=tokenizer.tokenize(contenidos[i][0])

"""Veamos el resultado"""

contenidos[701][0]

contenidosLSTM=contenidos

len(contenidos[701][0])

"""Ahora eliminamos aquellas palabras que no aportan significado y los dígitos que pueda haber en el contenido."""

nltk.download('stopwords')
STOPWORDS = set(stopwords.words("spanish"))

def filtro(tokens):
    """
    Filtra stopwords y digitos de una lista de tokens.
    """
    return [token for token in tokens if token not in STOPWORDS
            and not token.isdigit()]

for i in range(len(contenidos)):
  contenidos[i][0]=filtro(contenidos[i][0])

"""Veamos el resultado"""

contenidos[701][0]

len(contenidos[701][0])

"""Como vemos se ha reducido el número de palabras considerablemente tras el preprocesamiento y el filtro

Por último lematizamos las palabras, en busca de su raíz.
"""

stemmer = SnowballStemmer("spanish")

def stemming(tokens):
    """
    Reduce cada palabra de una lista dada a su raíz.
    """
    return [stemmer.stem(token) for token in tokens]

for i in range(len(contenidos)):
  contenidos[i][0]=stemming(contenidos[i][0])

"""Veamos el resultado"""

contenidos[701][0]

"""El número de palabras tiene que seguir siendo el mismo"""

len(contenidos[701][0])

tokens=[]
for i in range(len(contenidos)):
   tokens.append(contenidos[i][0])

len(tokens)

contador=0
for i in range(len(tokens)): contador+=len(tokens[i])
print("Número de tokens", contador)

diccionario=Dictionary(tokens)

len(diccionario)

"""Veamos los elementos del diccionario"""

list (diccionario.items ())

diccionario.filter_extremes(no_below=2, no_above = 0.66)
print(f'Número de tokens: {len(diccionario)}')

"""A continuación creamos el corpus, de forma que tendremos en los documentos cada palabra indexada y con el recuento del número de veces que dicha palabra aparece en el documento."""

# Creamos el corpus
corpus = [diccionario.doc2bow(contenidos[i][0]) for i in range(len(contenidos))]

len(corpus)

"""Veamos uno al azar"""

corpus[701]

"""


# MODELO LSA
"""

from gensim.models import LsiModel
from gensim.models.coherencemodel import CoherenceModel

def lsa_model(diccionario,corpus,num_topicos,palabras):
    # generate LSA model
    lsamodel = LsiModel(corpus, num_topics=num_topicos, id2word = diccionario,chunksize=800,random_seed=2, power_iters=10)  # train model
    print(lsamodel.print_topics(num_topics=num_topicos, num_words=palabras))
    return lsamodel

"""Buscamos el modelo que mayor coherencia tenga."""

from gensim.models.coherencemodel import CoherenceModel

def valores_coherencia_LSA(diccionario, corpus, noticias, limit, start, step):
  valores_coherencia_cv = []
  valores_coherencia_cuci = []
  valores_coherencia_umass = []
  lista_modelos = []
  for numero_topicos in range(start, limit, step):
    model=LsiModel(corpus, num_topics=numero_topicos, id2word = diccionario,chunksize=800,random_seed=2, power_iters=10)
    lista_modelos.append(model)
    coherencemodel_cv = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_v')
    valores_coherencia_cv.append(coherencemodel_cv.get_coherence())
    coherencemodel_cuci = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_uci')
    valores_coherencia_cuci.append(coherencemodel_cuci.get_coherence())
    coherencemodel_umass = CoherenceModel(model=model, corpus=corpus, dictionary=diccionario, coherence='u_mass')
    valores_coherencia_umass.append(coherencemodel_umass.get_coherence())
  return lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass

lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass = valores_coherencia_LSA(diccionario,corpus,[contenidos[i][0] for i in range(len(contenidos))],15,3,1)

for i in range(len(lista_modelos)):
  print("El modelo",lista_modelos[i],"tiene un valor de coherencia cv de ",valores_coherencia_cv[i],"tiene un valor de coherencia cuci de ",valores_coherencia_cuci[i],"tiene un valor de coherencia umass de ",valores_coherencia_umass[i])

"""Representamos gráficamente la evolución de las métricas de coherencia respecto al número de tópicos."""

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,valores_coherencia_cv,label="c_v")
plt.plot(numero_topicos,valores_coherencia_umass,label="u_mass")
plt.plot(numero_topicos,valores_coherencia_cuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

data = {
    'c_v': valores_coherencia_cv,
    'u_mass': valores_coherencia_umass,
    'c_uci': valores_coherencia_cuci,
}

df_lsa=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_lsa.loc[str(i+3)+" tópicos"]=[valores_coherencia_cv[i],valores_coherencia_umass[i],valores_coherencia_cuci[i]]

df_lsa

"""Entrenamos el modelo que presenta la mayor coherencia global y mostramos la distribución de sus tópicos."""

LSA=LsiModel(corpus, num_topics=4, id2word = diccionario,chunksize=800,random_seed=2, power_iters=10)

topicos = LSA.print_topics(num_words=8, num_topics=4)
for topico in topicos:
    print(topico)

"""# MODELO PLSA

Importamos la librería PLSA y sus métodos necesarios.
"""

!pip install plsa
from plsa import Pipeline, Visualize
from plsa.pipeline import DEFAULT_PIPELINE
from plsa.algorithms import PLSA
from plsa.corpus import Corpus
from plsa.preprocessors import remove_non_ascii,to_lower,remove_numbers,remove_tags,remove_punctuation,tokenize,RemoveStopwords,LemmatizeWords,remove_short_words

nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

"""Aplicamos el procesamiento adecuado y estándar de esta librería."""

contenidos_PLSA=[]
pipeline = Pipeline(*DEFAULT_PIPELINE)
for i in range(len(contenidosPLSA)):
  contenidos_PLSA.append(pipeline.process(contenidosPLSA[i][0]))

tokens=[]
for i in range(len(contenidos_PLSA)):
   tokens.append(contenidos_PLSA[i])

"""Realizamos comprobaciones"""

diccionario=Dictionary(tokens)
len(diccionario)

# Creamos el corpus
corpus = [diccionario.doc2bow(contenidos_PLSA[i]) for i in range(len(contenidos_PLSA))]

corpus_PLSA=Corpus([contenidosPLSA[i] for i in range(len(contenidosPLSA))],pipeline)

len(corpus_PLSA.vocabulary)

"""Buscamos el mejor modelo."""

numero_topicos=range(3,15,1)
coherencecv=[]
coherencecuci=[]
coherencecumass=[]
for i in numero_topicos:
  plsa = PLSA(corpus_PLSA, i, True)
  result = plsa.fit()
  palabras=[]
  palabras_topico=[]
  for i in result.word_given_topic:
    for j in range(len(i)):
      palabras_topico.append(i[j][0])
    palabras.append(palabras_topico)
    palabras_topico=[]
  cv = CoherenceModel(topics=palabras, texts=tokens, dictionary=diccionario,coherence='c_v')
  cuci = CoherenceModel(topics=palabras, texts=tokens, dictionary=diccionario, coherence='c_uci')
  cumass = CoherenceModel(topics=palabras, corpus=corpus, dictionary=diccionario, coherence='u_mass')
  coherencecv.append(cv.get_coherence())  # get coherence value
  coherencecuci.append(cuci.get_coherence())
  coherencecumass.append(cumass.get_coherence())
  palabras=[]

df_plsa=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_plsa.loc[str(i+3)+" tópicos"]=[coherencecv[i],coherencecumass[i],coherencecuci[i]]
df_plsa

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,coherencecv,label="c_v")
plt.plot(numero_topicos,coherencecumass,label="u_mass")
plt.plot(numero_topicos,coherencecuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

"""Se entrena el mejor modelo en base a las métricas de coherencia."""

plsa = PLSA(corpus_PLSA, 12, True)
result = plsa.fit()

result = plsa.best_of(5)

print(result.word_given_topic[0][0:8])
print(result.word_given_topic[1][0:8])
print(result.word_given_topic[2][0:8])
print(result.word_given_topic[3][0:8])
print(result.word_given_topic[4][0:8])
print(result.word_given_topic[5][0:8])
print(result.word_given_topic[6][0:8])
print(result.word_given_topic[7][0:8])
print(result.word_given_topic[8][0:8])
print(result.word_given_topic[9][0:8])
print(result.word_given_topic[10][0:8])
print(result.word_given_topic[11][0:8])

"""# MODELO LDA"""

from gensim.models import LdaModel

import gensim
from gensim.models.coherencemodel import CoherenceModel

"""Buscamos el mejor modelo."""

from gensim.models.coherencemodel import CoherenceModel

def valores_coherencia_LDA(diccionario, corpus, noticias, limit, start=2, step=2):
  valores_coherencia_cv = []
  valores_coherencia_cuci = []
  valores_coherencia_umass = []
  lista_modelos = []
  for numero_topicos in range(start, limit, step):
    model=LdaModel(corpus=corpus, id2word=diccionario,num_topics=numero_topicos, random_state=1, chunksize=800, passes=10, alpha='auto')
    lista_modelos.append(model)
    coherencemodel_cv = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_v')
    valores_coherencia_cv.append(coherencemodel_cv.get_coherence())
    coherencemodel_cuci = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_uci')
    valores_coherencia_cuci.append(coherencemodel_cuci.get_coherence())
    coherencemodel_umass = CoherenceModel(model=model, corpus=corpus, dictionary=diccionario, coherence='u_mass')
    valores_coherencia_umass.append(coherencemodel_umass.get_coherence())
  return lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass

lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass = valores_coherencia_LDA(diccionario,corpus,[contenidos[i][0] for i in range(len(contenidos))],15,3,1)

for i in range(len(lista_modelos)):
  print("El modelo",lista_modelos[i],"tiene un valor de coherencia cv de ",valores_coherencia_cv[i],"tiene un valor de coherencia cuci de ",valores_coherencia_cuci[i],"tiene un valor de coherencia umass de ",valores_coherencia_umass[i])

"""Representamos la evolución de las métricas en base al número de tópicos."""

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,valores_coherencia_cv,label="c_v")
plt.plot(numero_topicos,valores_coherencia_umass,label="u_mass")
plt.plot(numero_topicos,valores_coherencia_cuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

data = {
    'c_v': valores_coherencia_cv,
    'u_mass': valores_coherencia_umass,
    'c_uci': valores_coherencia_cuci,
}

df_lda=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_lda.loc[str(i+3)+" tópicos"]=[valores_coherencia_cv[i],valores_coherencia_umass[i],valores_coherencia_cuci[i]]

"""Sed muestran los valores de cada métrica para cada número de tópicos."""

df_lda

model=LdaModel(corpus=corpus, id2word=diccionario,num_topics=5, random_state=1, chunksize=800, passes=10, alpha='auto')

topicos = model.print_topics(num_words=8, num_topics=5)
for topico in topicos:
    print(topico)

"""Similitud entre textos (usando la distancia de Jensen-Shannon). Se propone como línea de investigación futura."""

from gensim.matutils import jensen_shannon

"""Usaremos la función que tiene gensim implementada"""

def calcular_distanciajensen_shannon(doc_1, doc_2):

    return jensen_shannon(doc_1, doc_2)

"""Por otro lado, se puede utilizar la siguiente función para sugerir noticias similares a un usuario que esté buscando una noticia en concreto"""

def similares(noticia, n_similares):
  distancias = [calcular_distanciajensen_shannon(lda.get_document_topics(corpus[noticia],
                                               minimum_probability=0), lda.get_document_topics(corpus[i],
                                               minimum_probability=0)) for i in range(len(corpus))]
  mas_similares = np.argsort(distancias)[0:n_similares] #usamos argsort porque queremos encontrar aquellas noticias con las distancias más bajas
  return mas_similares

"""Busquemos las 10 noticias más relacionadas con la informada, ponemos 11 ya que en principio la propia noticia tendrá la menor distancia con ella misma. Lo dejamos y no lo sacamos del bucle porque está bien para comprobar el resultado de la distancia."""

noticias_mas_similares=similares(701,11)

noticias_mas_similares

for i in noticias_mas_similares:
  print(enlaces_noticias[i])

"""# MODELO HDP

Se incorpora la librería y los métodos necesarios.
"""

from gensim.models import HdpModel
from gensim.models.coherencemodel import CoherenceModel

"""En este algoritmo el número de tópicos no es un parámetro de entrada necesario, sin embargo, se forzará que lo sea mediante el truncamiento del árbol jerárquico, que se realiza mediante el parámetro de entrada T de la función HdpModel.

---


"""

from gensim.models.coherencemodel import CoherenceModel

def valores_coherencia_HDP(diccionario, corpus, noticias, limit, start=2, step=2):
  valores_coherencia_cv = []
  valores_coherencia_cuci = []
  valores_coherencia_umass = []
  lista_modelos = []
  for numero_topicos in range(start, limit, step):
    model=HdpModel(corpus=corpus, id2word=diccionario,chunksize=800, random_state=2,max_chunks=10,T=numero_topicos)
    lista_modelos.append(model)
    coherencemodel_cv = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_v')
    valores_coherencia_cv.append(coherencemodel_cv.get_coherence())
    coherencemodel_cuci = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_uci')
    valores_coherencia_cuci.append(coherencemodel_cuci.get_coherence())
    coherencemodel_umass = CoherenceModel(model=model, corpus=corpus, dictionary=diccionario, coherence='u_mass')
    valores_coherencia_umass.append(coherencemodel_umass.get_coherence())
  return lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass

lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass = valores_coherencia_HDP(diccionario,corpus,[contenidos[i][0] for i in range(len(contenidos))],15,3,1)

for i in range(len(lista_modelos)):
  print("El modelo",lista_modelos[i],"tiene un valor de coherencia cv de ",valores_coherencia_cv[i],"tiene un valor de coherencia cuci de ",valores_coherencia_cuci[i],"tiene un valor de coherencia umass de ",valores_coherencia_umass[i])

"""Se muestra la evolución de las métricas de coherencia en base a los diferentes modelo variando el número de tópicos truncados."""

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,valores_coherencia_cv,label="c_v")
plt.plot(numero_topicos,valores_coherencia_umass,label="u_mass")
plt.plot(numero_topicos,valores_coherencia_cuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

data = {
    'c_v': valores_coherencia_cv,
    'u_mass': valores_coherencia_umass,
    'c_uci': valores_coherencia_cuci,
}

df_HDP=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_HDP.loc[str(i+3)+" tópicos"]=[valores_coherencia_cv[i],valores_coherencia_umass[i],valores_coherencia_cuci[i]]

df_HDP

model=HdpModel(corpus=corpus, id2word=diccionario,chunksize=800, random_state=2,max_chunks=10,T=3)

"""Se muestra la distribución de los tópicos del mejor modelo."""

model.print_topics(3,8)

"""# MODELO NNMF"""

from gensim.models.nmf import Nmf
from gensim.models.coherencemodel import CoherenceModel

"""Buscamos el mejor modelo en base a las métricas de coherencia."""

def valores_coherencia_NNMF(diccionario, corpus, noticias, start, limit, step):
  valores_coherencia_cv = []
  valores_coherencia_cuci = []
  valores_coherencia_umass = []
  lista_modelos = []
  for numero_topicos in range(start, limit, step):
    model=Nmf(corpus=corpus, id2word=diccionario, num_topics=numero_topicos,chunksize=800, random_state=2, passes=10,normalize=True)
    lista_modelos.append(model)
    coherencemodel_cv = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_v')
    valores_coherencia_cv.append(coherencemodel_cv.get_coherence())
    coherencemodel_cuci = CoherenceModel(model=model, texts=tokens, dictionary=diccionario,coherence='c_uci')
    valores_coherencia_cuci.append(coherencemodel_cuci.get_coherence())
    coherencemodel_umass = CoherenceModel(model=model, corpus=corpus, dictionary=diccionario, coherence='u_mass')
    valores_coherencia_umass.append(coherencemodel_umass.get_coherence())
  return lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass

lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass = valores_coherencia_NNMF(diccionario,corpus,[contenidos[i][0] for i in range(len(contenidos))],3,15,1)

for i in range(len(lista_modelos)):
  print("El modelo",lista_modelos[i],"tiene un valor de coherencia cv de ",valores_coherencia_cv[i],"tiene un valor de coherencia cuci de ",valores_coherencia_cuci[i],"tiene un valor de coherencia umass de ",valores_coherencia_umass[i])

"""Representemos los valores de coherencia obtenidos atendiendo al número de tópicos."""

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,valores_coherencia_cv,label="c_v")
plt.plot(numero_topicos,valores_coherencia_umass,label="u_mass")
plt.plot(numero_topicos,valores_coherencia_cuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

data = {
    'c_v': valores_coherencia_cv,
    'u_mass': valores_coherencia_umass,
    'c_uci': valores_coherencia_cuci,
}

df_nnmf=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_nnmf.loc[str(i+3)+" tópicos"]=[valores_coherencia_cv[i],valores_coherencia_umass[i],valores_coherencia_cuci[i]]

df_nnmf

"""Entrenamos el mejor modelo y mostrmos los tópicos y su distribución."""

NNMF=Nmf(corpus=corpus, id2word=diccionario, num_topics=7,chunksize=800, random_state=2, passes=10,normalize=True)

topicos = NNMF.print_topics(num_words=8, num_topics=7)
for topico in topicos:
    print(topico)

"""# MODELO CTM

Se importan las librerías y paquetes necesarios
"""

!pip install tomotopy
import tomotopy as tp
from tomotopy import CTModel, utils, coherence

from tomotopy.utils import Corpus
corpus_ctm = tp.utils.Corpus()
for i in tokens:
    corpus_ctm.add_doc(i)

from gensim.models.coherencemodel import CoherenceModel

def valores_coherencia_CTM(diccionario, corpus, noticias, limit, start, step):
  valores_coherencia_cv = []
  valores_coherencia_cuci = []
  valores_coherencia_umass = []
  lista_modelos = []
  for numero_topicos in range(start, limit, step):
    ctm=CTModel(tw=tp.TermWeight.PMI,k=numero_topicos, smoothing_alpha=0.1, eta=0.01, seed=2, corpus=corpus_ctm)
    ctm.train(10)
    lista_modelos.append(ctm)
    coherencemodel_cv = tp.coherence.Coherence(ctm, coherence='c_v')
    valores_coherencia_cv.append(coherencemodel_cv.get_score())
    coherencemodel_cuci = tp.coherence.Coherence(ctm, coherence='c_uci')
    valores_coherencia_cuci.append(coherencemodel_cuci.get_score())
    coherencemodel_umass =tp.coherence.Coherence(ctm, coherence='u_mass')
    valores_coherencia_umass.append(coherencemodel_umass.get_score())
  return lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass

"""Se busca el mejor modelo atendiendo al número de tópicos."""

lista_modelos, valores_coherencia_cv,valores_coherencia_cuci,valores_coherencia_umass = valores_coherencia_CTM(diccionario,corpus,[contenidos[i][0] for i in range(len(contenidos))],15,3,1)

for i in range(len(lista_modelos)):
  print("El modelo",lista_modelos[i],"tiene un valor de coherencia cv de ",valores_coherencia_cv[i],"tiene un valor de coherencia cuci de ",valores_coherencia_cuci[i],"tiene un valor de coherencia umass de ",valores_coherencia_umass[i])

"""Se muestra la evolución de las métricas."""

# Show graph
import matplotlib.pyplot as plt
import numpy as np
numero_topicos=range(3,15,1)

plt.figure(figsize=(10,8))
plt.plot(numero_topicos,valores_coherencia_cv,label="c_v")
plt.plot(numero_topicos,valores_coherencia_umass,label="u_mass")
plt.plot(numero_topicos,valores_coherencia_cuci,label="c_uci")
plt.title("Valores de coherencia comparando modelos")
plt.xlabel("Número de tópicos")
plt.ylabel("Valor coherencia")
plt.legend()
plt.xticks(rotation=45)

plt.show()

data = {
    'c_v': valores_coherencia_cv,
    'u_mass': valores_coherencia_umass,
    'c_uci': valores_coherencia_cuci,
}

df_ctm=pd.DataFrame(columns=['c_v', 'u_mass', 'c_uci'])
for i in range(len(numero_topicos)):
  df_ctm.loc[str(i+3)+" tópicos"]=[valores_coherencia_cv[i],valores_coherencia_umass[i],valores_coherencia_cuci[i]]

df_ctm

"""Se entrena el meejor modelo para los dos tipos de matriz de pesos posibles."""

ctm=CTModel(tw=tp.TermWeight.ONE, min_cf=0, min_df=0, rm_top=0, k=3, smoothing_alpha=0.1, eta=0.01, seed=2, corpus=corpus_ctm)
ctm.train(100)
print(ctm.get_topic_words(0,8))
print(ctm.get_topic_words(1,8))
print(ctm.get_topic_words(2,8))

ctm=CTModel(tw=tp.TermWeight.PMI, min_cf=0, min_df=0, rm_top=0, k=3, smoothing_alpha=0.1, eta=0.01, seed=2, corpus=corpus_ctm)
ctm.train(100)
print(ctm.get_topic_words(0,8))
print(ctm.get_topic_words(1,8))
print(ctm.get_topic_words(2,8))

from tomotopy import Document
etiquetas=[]
for i in ctm.docs:
  etiquetas.append(Document.get_topics(i)[0][0])

"""# EnsembleLDA

Se importan los paquetes y métodos necesarios.
"""

from gensim.models import EnsembleLda

"""Se buscan los tópicos, poniendo el límite superior en los 10 tópicos más coherentes."""

eLDA = EnsembleLda(corpus=corpus, id2word=diccionario, num_topics=10, num_models=30, passes=10)

"""Se muestran los tópicos obtenidos."""

eLDA.print_topics(5)

coherencemodel_cv = CoherenceModel(model=eLDA, texts=tokens, dictionary=diccionario,coherence='c_v')
valor_coherencia_cv=coherencemodel_cv.get_coherence()
coherencemodel_cuci = CoherenceModel(model=eLDA, texts=tokens, dictionary=diccionario,coherence='c_uci')
valor_coherencia_cuci=coherencemodel_cuci.get_coherence()
coherencemodel_umass = CoherenceModel(model=eLDA, corpus=corpus, dictionary=diccionario, coherence='u_mass')
valor_coherencia_umass=coherencemodel_umass.get_coherence()

"""Se obtienen las métricas."""

print(valor_coherencia_cv)
print(valor_coherencia_cuci)
print(valor_coherencia_umass)

"""# Redes neuronales recurrentes (LSTM bidireccional)

Lo emplearemos para el diario La Razón, para ello extraeremos 280 noticias de cada una de las 9 secciones principales de su web.Para ello extraeremos noticias de las 20 primeras páginas de cada sección, pues hay 14 noticias en cada página. En total extaeremos 2520 noticias.
"""

temas=["espana","internacional","economia","sociedad","opinion","deportes","cultura","salud","gente"]
enlace_noticias=[]
for t in temas:
  URL = "https://www.larazon.es/"+t+"/"
  page = requests.get(URL)
  soup = BeautifulSoup(page.content, "html.parser")
  extracción1=soup.find_all("section", class_="row distributiva")
  for i in range(len(extracción1)):
    extracción2=extracción1[i].find_all("a", class_="article__media video")
    for j in range(len(extracción2)):
      enlace_noticias.append((extracción2[j].attrs["href"],t))
  print(len(enlace_noticias))
  for p in range(2,21):
    URL = "https://www.larazon.es/"+t+"/"+str(p)+"/"
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    extracción1=soup.find_all("section", class_="row distributiva")
    for i in range(len(extracción1)):
      extracción2=extracción1[i].find_all("a", class_="article__media video")
      for j in range(len(extracción2)):
        enlace_noticias.append((extracción2[j].attrs["href"],t))
    print(len(enlace_noticias))

enlace_noticias

lista = enlace_noticias
mi_path = "/content/enlaces.txt"

with open(mi_path, 'a+') as f:
    for i in lista:
        f.write(str(i))

len(enlace_noticias)

contenidos=[]
contador=0


for i in enlace_noticias:
  if len(i[0])>2 and "directo" not in str(i[0]):
    print(i[0])
    page = requests.get(i[0])
    soup = BeautifulSoup(page.content, "html.parser")
    if soup.find("script", type="application/ld+json")!=None:
      if "articleBody" in json.loads(soup.find("script", type="application/ld+json").text):
        contenidos.append([json.loads(soup.find("script", type="application/ld+json").text)["articleBody"],i[1]])
        contador+=1
        print(contador)

"""Solo quitaremos los caracteres especiales y elementos forzados de HTML, es decir realizaremos la acción de nuestra función "preprocesamiento". Sin embargo, no eliminaremos stopwords ni realizaremos "stemming" porque en LSTM interesa mantener la coherencia y trazabilidad de la oración al completo."""

for i in range(len(contenidos)):
  contenidos[i][0]=preprocesamiento(contenidos[i][0])

import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional,BatchNormalization,Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.optimizers import Adam

df = pd.DataFrame(contenidos)

print(df)

df.columns = ['contenido', 'etiqueta']
df

mapeos = {
  0: "espana",
  1: "internacional",
  2: "economia",
  3: "sociedad",
  4: "opinion",
  5: "deportes",
  6: "cultura",
  7: "salud",
  8: "gente"
}

df.drop(df[(df["contenido"].str.len()<200)].index,inplace=True)

df['etiqueta'] = df['etiqueta'].replace(
  # Reemplaza cada valor por el valor en la segunda lista en el mismo índice del match.
  mapeos.values(), # Lista de Strings
  mapeos.keys() # Lista de Números
)
df

train_data, test_data = train_test_split(df[['contenido', 'etiqueta']], test_size=0.1)

training_sentences = list(train_data['contenido'])
training_labels = list(train_data['etiqueta'])

testing_sentences = list(test_data['contenido'])
testing_labels = list(test_data['etiqueta'])
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

from keras.utils import to_categorical
training_labels_final=to_categorical(training_labels)
testing_labels_final=to_categorical(testing_labels)

testing_labels_final[0]

vocabulario= 20000   # limit vector of words to the top 20,000 words
embedding_dim = 64
max_length = 200
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocabulario, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

testing_padded[0]

!pip freeze
!pip install -q -U keras-tuner
import kerastuner as kt

def model_builder(hp):

  hp_units = hp.Int('units', min_value = 32, max_value = 128, step = 32)
  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])
  hp_rate = hp.Float('rate', min_value = 0.2, max_value = 0.5, step = 0.1)
  hp_l2=hp.Choice('l2', values = [1e-2,2e-2, 1e-3,2e-3, 1e-4,2e-4])

  # Creamos la arquitectura LSTM
  embedding_vector_length = 64
  model = Sequential()
  model.add(Embedding(vocabulario, embedding_vector_length, input_length=max_length))
  model.add(Bidirectional(LSTM(90,recurrent_dropout=0.25,dropout=0.1)))
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(9, activation='softmax'))
  # Compilamos el modelo
  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = hp_learning_rate), metrics=['accuracy'])
  return model

tuner = kt.Hyperband(model_builder,
                     objective = 'val_accuracy',
                     max_epochs = 10,
                     factor = 3,
                     directory = 'my_dir',
                     project_name = 'intro_to_kt')

import tensorflow as tf
import IPython

class ClearTrainingOutput(tf.keras.callbacks.Callback):
  def on_train_end(*args, **kwargs):
    IPython.display.clear_output(wait = True)


from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,Callback
parada_temprana = EarlyStopping(monitor='val_loss',patience=2, verbose=1,min_delta=1e-4)
reducir_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=1,verbose=1)
checkpoint = ModelCheckpoint(filepath='mejormodelo.h5',monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')
callbacks_list = [parada_temprana, reducir_lr,ClearTrainingOutput(),checkpoint]

tuner.search(padded, training_labels_final, epochs=10, validation_split=0.10, batch_size=32, callbacks = [callbacks_list])

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]

print(f"""
La optimización ha terminado. El número óptimo de neuronas en las capas densas es de {best_hps.get('units')} y , el porcentaje óptimo de dropout es {best_hps.get('rate')*100} y la tasa
optima de aprendizaje es de {best_hps.get('learning_rate')}.
""")

"""Entrenamos el modelo que mejor precisión obtuvo"""

# Creamos la arquitectura LSTM
embedding_vector_length = 64
model = Sequential()
model.add(Embedding(vocabulario, embedding_vector_length, input_length=max_length))
model.add(Bidirectional(LSTM(90,recurrent_dropout=0.25,dropout=0.1)))
model.add(Dense(units = 32, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu')),
model.add(BatchNormalization()),
model.add(Dropout(rate=0.2)),
model.add(Dense(units = 32, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu')),
model.add(BatchNormalization()),
model.add(Dropout(rate=0.2)),
model.add(Dense(9, activation='softmax'))
print(model.summary())

precisiones=[]
contador=0
# Entrenamos el modelo
for i in [8,10,12,14,16] :
  for j in [32,64,128,256]:
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = 0.001), metrics=['accuracy'])
    model.fit(padded, training_labels_final, epochs=i, validation_split=0.1, batch_size=j,callbacks = [callbacks_list])
    # Evaluamos el modelo
    scores = model.evaluate(testing_padded, testing_labels_final, verbose=0)
    precisiones.append(scores[1]*100)
    print("Accuracy: %.2f%% para %a épocas y tamaño de lote %a" % (precisiones[contador],i,j))
    contador+=1

"""# MODELO LLDA

Se importan los paquetes necesarios
"""

from tomotopy import LLDAModel

llda=LLDAModel(tw=tp.TermWeight.PMI, k=3, alpha=0.1, eta=0.01, seed=2)
for i in range(len(tokens[0:624])):
  llda.add_doc(tokens[i], [str(etiquetas[i])])
llda.train(100)

for i in range(len(tokens[0:624])):
  llda.add_doc(tokens[i], [str(etiquetas[i])])

predicciones=[]
for i in tokens[624:]:
  doc_nuevo = llda.make_doc(i)
  topic_dist= llda.infer(doc_nuevo)
  prediccion=topic_dist[0].argmax()
  predicciones.append(prediccion)

len(predicciones)

"""Se realiza la matriz de confusión para evaluar la precisión y desempeño del modelo LLDA sobre las etiquetas proporcionadas por cada uno de los dos modelos no supervisados escogidos."""

import matplotlib.pyplot as plt
import numpy
from sklearn import metrics
# Crear la matriz de confusión utilizando sklearn
confusion_matrix = metrics.confusion_matrix(etiquetas[624:], predicciones)

# Crear la visualización de la matriz de confusión
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2])

# Graficar la matriz de confusión
cm_display.plot()
plt.show()

etiquetas=[]
for i in result.topic_given_doc:
  etiquetas.append(i.argmax())

llda=LLDAModel(tw=tp.TermWeight.PMI, min_cf=0, min_df=0, rm_top=0, k=12, alpha=0.1, eta=0.01, seed=2)

for i in range(len(tokens[0:624])):
  llda.add_doc(tokens[i], [str(etiquetas[i])])

llda.train(10)

predicciones=[]
for i in tokens[624:]:
  doc_nuevo = llda.make_doc(i)
  topic_dist= llda.infer(doc_nuevo)
  prediccion=topic_dist[0].argmax()
  predicciones.append(prediccion)

import matplotlib.pyplot as plt
import numpy
from sklearn import metrics
# Crear la matriz de confusión utilizando sklearn
confusion_matrix = metrics.confusion_matrix(etiquetas[624:], predicciones)

# Crear la visualización de la matriz de confusión
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])

# Graficar la matriz de confusión
cm_display.plot()
plt.show()

"""# Modelo SLDA

Se importan los paquetes necesarios.
"""

from tomotopy import SLDAModel

from tomotopy import Document
etiquetas=[]
for i in ctm.docs:
  etiquetas.append(Document.get_topics(i)[0][0])

slda=SLDAModel(tw=tp.TermWeight.ONE, k=3, vars='l', alpha=0.1, eta=0.01)
for i in range(len(tokens[0:624])):
  slda.add_doc(tokens[i], [etiquetas[i]])
slda.train(100)

predicciones=[]
for i in tokens[624:]:
  doc_nuevo = slda.make_doc(i)
  topic_dist= slda.infer(doc_nuevo)
  prediccion=topic_dist[0].argmax()
  predicciones.append(prediccion)

"""Se realiza la matriz de confusión para evaluar la precisión y desempeño del modelo sLDA sobre las etiquetas proporcionadas por cada uno de los dos modelos no supervisados escogidos."""

import matplotlib.pyplot as plt
import numpy
from sklearn import metrics
# Crear la matriz de confusión utilizando sklearn
confusion_matrix = metrics.confusion_matrix(etiquetas[624:], predicciones)

# Crear la visualización de la matriz de confusión
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2])

# Graficar la matriz de confusión
cm_display.plot()
plt.show()

etiquetas=[]
for i in result.topic_given_doc:
  etiquetas.append(i.argmax())

slda=SLDAModel(tw=tp.TermWeight.ONE, min_cf=0, min_df=0, rm_top=0, k=12, vars='l', alpha=0.1, eta=0.01)
for i in range(len(tokens[0:624])):
  slda.add_doc(tokens[i], [etiquetas[i]])

slda.train(100)

predicciones=[]
for i in tokens[624:]:
  doc_nuevo = slda.make_doc(i)
  topic_dist= slda.infer(doc_nuevo)
  prediccion=topic_dist[0].argmax()
  predicciones.append(prediccion)

import matplotlib.pyplot as plt
import numpy
from sklearn import metrics
# Crear la matriz de confusión utilizando sklearn
confusion_matrix = metrics.confusion_matrix(etiquetas[624:], predicciones)

# Crear la visualización de la matriz de confusión
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2,3,4,5,6,7,8,9,10,11])

# Graficar la matriz de confusión
cm_display.plot()
plt.show()

"""# LSTM VIU CTM

Análogo que para el periódico La Razón pero usando como etiquetas las dadas por el modelo CTM.

Solo quitaremos los caracteres especiales y elementos forzados de HTML, es decir realizaremos la acción de nuestra función "preprocesamiento". Sin embargo, no eliminaremos stopwords ni realizaremos "stemming" porque en LSTM interesa mantener la coherencia y trazabilidad de la oración al completo.
"""

import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional,BatchNormalization,Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.optimizers import Adam

df = pd.DataFrame(contenidosLSTM)

print(df)

from tomotopy import Document
etiquetas=[]
for i in ctm.docs:
  etiquetas.append(Document.get_topics(i)[0][0])

df['etiqueta']=etiquetas
df.columns = ['contenido', 'etiqueta']
df

df.drop(df[(df["contenido"].str.len()<100)].index,inplace=True)

train_data, test_data = train_test_split(df[['contenido', 'etiqueta']], test_size=0.1)

training_sentences = list(train_data['contenido'])
training_labels = list(train_data['etiqueta'])

testing_sentences = list(test_data['contenido'])
testing_labels = list(test_data['etiqueta'])
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

from keras.utils import to_categorical
training_labels_final=to_categorical(training_labels)
testing_labels_final=to_categorical(testing_labels)

testing_labels_final[0]

vocabulario= 20000   # limit vector of words to the top 20,000 words
embedding_dim = 64
max_length = 200
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocabulario, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

testing_padded[0]

!pip freeze
!pip install -q -U keras-tuner
import kerastuner as kt

"""Construcción del modelo."""

def model_builder3(hp):

  hp_units = hp.Int('units', min_value = 32, max_value = 128, step = 32)
  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])
  hp_rate = hp.Float('rate', min_value = 0.2, max_value = 0.5, step = 0.1)
  hp_l2=hp.Choice('l2', values = [1e-2,2e-2, 1e-3,2e-3, 1e-4,2e-4])

  # Creamos la arquitectura LSTM
  embedding_vector_length = 64
  model = Sequential()
  model.add(Embedding(vocabulario, embedding_vector_length, input_length=max_length))
  model.add(Bidirectional(LSTM(90,recurrent_dropout=0.25,dropout=0.1)))
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(3, activation='softmax'))
  # Compilamos el modelo
  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = hp_learning_rate), metrics=['accuracy'])
  return model

"""Optimización del modelo."""

tuner3 = kt.Hyperband(model_builder3,
                     objective = 'val_accuracy',
                     max_epochs = 10,
                     factor = 3,
                     directory = 'my_dir3',
                     project_name = 'intro_to_kt')

import tensorflow as tf
import IPython

class ClearTrainingOutput(tf.keras.callbacks.Callback):
  def on_train_end(*args, **kwargs):
    IPython.display.clear_output(wait = True)


from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,Callback
parada_temprana = EarlyStopping(monitor='val_loss',patience=3, verbose=1,min_delta=1e-4)
reducir_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=2,verbose=1)
checkpoint = ModelCheckpoint(filepath='mejormodelo.h5',monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')
callbacks_list = [parada_temprana, reducir_lr,ClearTrainingOutput(),checkpoint]

tuner3.search(padded, training_labels_final, epochs=10, validation_split=0.10, batch_size=32, callbacks = [callbacks_list])

# Get the optimal hyperparameters
best_hps = tuner3.get_best_hyperparameters(num_trials = 1)[0]

print(f"""
La optimización ha terminado. El número óptimo de neuronas en las capas densas es de {best_hps.get('units')} , el porcentaje óptimo de dropout es {best_hps.get('rate')}, el peso óptimo de kernel es {best_hps.get('l2')} y la tasa
optima de aprendizaje es de {best_hps.get('learning_rate')}.
""")

"""# LSTM VIU PLSA

Análogo que para el periódico La Razón pero usando como etiquetas las dadas por el modelo PLSA.

Solo quitaremos los caracteres especiales y elementos forzados de HTML, es decir realizaremos la acción de nuestra función "preprocesamiento". Sin embargo, no eliminaremos stopwords ni realizaremos "stemming" porque en LSTM interesa mantener la coherencia y trazabilidad de la oración al completo.
"""

import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional,BatchNormalization,Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.optimizers import Adam

df = pd.DataFrame(contenidosLSTM)

print(df)

from tomotopy import Document
etiquetas=[]
for i in range(len(result.topic_given_doc)):
  etiquetas.append(result.topic_given_doc[i].argmax())

df['etiqueta']=etiquetas
df.columns = ['contenido', 'etiqueta']
df

df.drop(df[(df["contenido"].str.len()<100)].index,inplace=True)

"""Separamos en conjunto de entrenamiento y de test."""

train_data, test_data = train_test_split(df[['contenido', 'etiqueta']], test_size=0.1)

training_sentences = list(train_data['contenido'])
training_labels = list(train_data['etiqueta'])

testing_sentences = list(test_data['contenido'])
testing_labels = list(test_data['etiqueta'])
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

from keras.utils import to_categorical
training_labels_final=to_categorical(training_labels)
testing_labels_final=to_categorical(testing_labels)

testing_labels_final[0]

vocabulario= 20000   # limit vector of words to the top 20,000 words
embedding_dim = 64
max_length = 200
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocabulario, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

testing_padded[0]

!pip freeze
!pip install -q -U keras-tuner
import kerastuner as kt

"""Construcción del modelo."""

def model_builder4(hp):

  hp_units = hp.Int('units', min_value = 32, max_value = 128, step = 32)
  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])
  hp_rate = hp.Float('rate', min_value = 0.2, max_value = 0.5, step = 0.1)
  hp_l2=hp.Choice('l2', values = [1e-2,2e-2, 1e-3,2e-3, 1e-4,2e-4])

  # Creamos la arquitectura LSTM
  embedding_vector_length = 64
  model = Sequential()
  model.add(Embedding(vocabulario, embedding_vector_length, input_length=max_length))
  model.add(Bidirectional(LSTM(90,recurrent_dropout=0.25,dropout=0.1)))
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(units = hp_units, kernel_regularizer=tf.keras.regularizers.l2(hp_l2), activation='relu')),
  model.add(BatchNormalization()),
  model.add(Dropout(rate=hp_rate)),
  model.add(Dense(12, activation='softmax'))
  # Compilamos el modelo
  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = hp_learning_rate), metrics=['accuracy'])
  return model

"""Optimización de hiperparámetros."""

tuner4 = kt.Hyperband(model_builder4,
                     objective = 'val_accuracy',
                     max_epochs = 10,
                     factor = 3,
                     directory = 'my_dir4',
                     project_name = 'intro_to_kt')

import tensorflow as tf
import IPython

class ClearTrainingOutput(tf.keras.callbacks.Callback):
  def on_train_end(*args, **kwargs):
    IPython.display.clear_output(wait = True)


from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,Callback
parada_temprana = EarlyStopping(monitor='val_loss',patience=3, verbose=1,min_delta=1e-4)
reducir_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=2,verbose=1)
checkpoint = ModelCheckpoint(filepath='mejormodelo.h5',monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')
callbacks_list = [parada_temprana, reducir_lr,ClearTrainingOutput(),checkpoint]

tuner4.search(padded, training_labels_final, epochs=10, validation_split=0.10, batch_size=32, callbacks = [callbacks_list])

# Get the optimal hyperparameters
best_hps = tuner4.get_best_hyperparameters(num_trials = 1)[0]

print(f"""
La optimización ha terminado. El número óptimo de neuronas en las capas densas es de {best_hps.get('units')} , el porcentaje óptimo de dropout es {best_hps.get('rate')}, el peso óptimo de kernel es {best_hps.get('l2')} y la tasa
optima de aprendizaje es de {best_hps.get('learning_rate')}.
""")

MODEL = tuner4.hypermodel.build(best_hps)
MODEL.summary()